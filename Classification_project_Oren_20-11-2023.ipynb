{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab82ae3-2056-4d0a-b9bb-41381ae2b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20-11-2023, Oren - Classification Project Notebook\n",
    "# Re-Starting using existing sklearn pre-processing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f07b91-371e-4668-a32f-ba9ec368cd81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier, \\\n",
    "    AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, roc_curve, recall_score, precision_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def csv_drive_path_generatoer(url):\n",
    " '''\n",
    " Help in read csv file directly from google drive.\n",
    " Make sure the csv format is standard.\n",
    " url:str - path to csv file example:\n",
    "   url = 'https://drive.google.com/file/d/126JPZ3lYwdLyJ2d_7jxM9jMtZaOlF-Ld/view?usp=sharing'\n",
    " return : str\n",
    " '''\n",
    " path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
    " return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f0fa29-8a47-4249-b722-f8bb4722c4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "link_X_train = 'https://drive.google.com/file/d/11Dgctv-N6z3ugQOpRrFNKVP5NNqPPWxK/view?usp=drive_link'\n",
    "link_y_train = 'https://drive.google.com/file/d/1NDF2aCymR4zjYK9mLYNRwIdB_EyE5ZHl/view?usp=drive_link'\n",
    "\n",
    "link_X_test = 'https://drive.google.com/file/d/1ZRg80tYdBO1pcd_6R1m_tiijZSfDXMZ6/view?usp=drive_link'\n",
    "\n",
    "path_X_train = csv_drive_path_generatoer(link_X_train)\n",
    "path_y_train = csv_drive_path_generatoer(link_y_train)\n",
    "\n",
    "path_X_test = csv_drive_path_generatoer(link_X_test)\n",
    "\n",
    "X_train = read_csv(path_X_train, index_col = 0)\n",
    "y_train = read_csv(path_y_train, index_col = 0)\n",
    "\n",
    "X_test = read_csv(path_X_test, index_col = 0)\n",
    "\n",
    "# Drop the extra 'id' column\n",
    "X_train = X_train.drop(['id'], axis = 1)\n",
    "X_test = X_test.drop(['id'], axis = 1)\n",
    "\n",
    "y_test_dummy = pd.DataFrame(data = np.zeros(len(X_test.index)), index = X_test.index, dtype = 'int64', columns = ['y_test_dummy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0464a65b-9687-4956-a183-b75885838a06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3722 entries, 1847 to 2575\n",
      "Data columns (total 9 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Education                  3722 non-null   object \n",
      " 1   JoiningYear                3722 non-null   int64  \n",
      " 2   City                       3722 non-null   object \n",
      " 3   PaymentTier                3722 non-null   int64  \n",
      " 4   Age                        3720 non-null   float64\n",
      " 5   Gender                     3720 non-null   object \n",
      " 6   EverBenched                3722 non-null   object \n",
      " 7   ExperienceInCurrentDomain  3722 non-null   int64  \n",
      " 8   Race                       3722 non-null   object \n",
      "dtypes: float64(1), int64(3), object(5)\n",
      "memory usage: 290.8+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9d980fb-124c-49e2-ae08-d6de5e5ed62d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defines custom transformer for feature engineering \n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        self.y = y\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, column = 'JoiningYear', new_feature = 'Tenure', current_year = 2023, index = 'id'):\n",
    "        # Check if 'JoiningYear' is present in the input data\n",
    "        if column not in X.columns:\n",
    "            raise ValueError(f\"Column {column} not found in input data.\")\n",
    "        \n",
    "        X[new_feature] = current_year - X[column]\n",
    "        \n",
    "        X = X.drop([column], axis = 1)\n",
    "        return X\n",
    "    \n",
    "# Defines a custom transformer to drop rows with null values based on a column_name\n",
    "\n",
    "class RemoveNullRowsTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, null_val_columns = ['Age']):\n",
    "        self.null_val_columns = null_val_columns\n",
    "        # self.index = index\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        self.y = y\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "    # joining X and y before dropping rows with null values\n",
    "        if self.y is not None:\n",
    "            y = self.y\n",
    "            # X = X.set_index(self.index)\n",
    "            # y = y.set_index(self.index)\n",
    "            X = X.join(y)\n",
    "            \n",
    "        return X.dropna(subset= self.null_val_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1dfc5b1-f725-41a4-8c5f-b27c7b95b5d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fe = FeatureEngineerAndNullsDropper()\n",
    "# # print(X_train.info())\n",
    "# fe.fit_transform(X_train.join(y_train)).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8621c541-633a-4948-9349-4f4e142552a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mastery code...\n",
    "# define model\n",
    "# model = LogisticRegression()\n",
    "# model = DecisionTreeClassifier()\n",
    "\n",
    "# Specify the order of categories for the 'Education' feature\n",
    "Education_order = ['Bachelors', 'Masters', 'PHD']\n",
    "\n",
    "# Specify the target columns per Transformer\n",
    "null_val_columns = ['Age','Gender']\n",
    "column = ['JoiningYear']\n",
    "ordinal_category_features = ['Education']\n",
    "non_binary_categorical_features = ['City', 'Race']\n",
    "binary_categorical_features = ['Gender','EverBenched'] \n",
    "\n",
    "# Define different types of transformer classes\n",
    "remove_null_rows_transformer = RemoveNullRowsTransformer()\n",
    "feature_engineer_transformer = FeatureEngineer()\n",
    "binary_categorical_transformer = OneHotEncoder(drop='if_binary')\n",
    "non_binary_categorical_transformer = OneHotEncoder()\n",
    "ordinal_transformer = OrdinalEncoder(categories=[Education_order])\n",
    "\n",
    "# Define a null rows Transformer\n",
    "remove_null_rows_transformer = RemoveNullRowsTransformer()\n",
    "\n",
    "# Define a ColumnTransformer (for the rest of the pre-processing steps)\n",
    "column_transformer = ColumnTransformer(\n",
    "        transformers= [\n",
    "            # ('remove_null_rows_transformer',remove_null_rows_transformer, null_val_columns),\n",
    "        ('nulls_droper_engineer',feature_engineer_transformer, column),\n",
    "        ('binary_cat', binary_categorical_transformer, binary_categorical_features),\n",
    "        ('non_binary_cat', non_binary_categorical_transformer, non_binary_categorical_features),\n",
    "        ('ordinal_cat', ordinal_transformer, ordinal_category_features)], \n",
    "        remainder='passthrough') # passthrough all columns not explicitly transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da89a779-b77e-48b2-84cf-a0a96c37a7ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a pipeline\u001b[39;00m\n\u001b[0;32m      5\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m      6\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnull_remover\u001b[39m\u001b[38;5;124m'\u001b[39m, remove_null_rows_transformer),  \u001b[38;5;66;03m# First transformer step\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     \u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolumn_transformer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcolumn_transformer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Second transformer step\u001b[39;00m\n\u001b[0;32m      9\u001b[0m ])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Fit the pipeline on the training data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "# Define a model\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('null_remover', remove_null_rows_transformer),  # First transformer step\n",
    "    ('column_transformer',column_transformer)\n",
    "    ('model',model)  # Second transformer step\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predict on the test data\n",
    "y_pred = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "214ce1be-76a3-4fe5-af4c-3ef7ecb0cd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931, 14)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb3f2f6-4199-49c2-8b8f-9ba5547f6aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model (replace with your actual evaluation metric)\n",
    "# Here, I'm using cross_val_score as an example\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_cv_score = -cv_scores.mean()\n",
    "\n",
    "print(f'Mean Cross-Validated MSE: {mean_cv_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1169cb5e-66ae-4fde-a12b-4c271e52f220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f4526c-82dd-4168-be41-32fbfb0ddb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689dfc5-0e07-462e-9a3e-e8f5f4f081bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e825928-d740-47eb-87ef-4ff339804486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(X_train, X_test, y_train, y_test, ensemble_classifier):\n",
    "    # generate a no skill prediction (majority class)\n",
    "    ns_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "    # clf = DecisionTreeClassifier()\n",
    "    # model_a_bagging = BaggingClassifier(estimator = base_model, n_estimators= 100)\n",
    "\n",
    "    # fitting the models\n",
    "    ensemble_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # predict probabilities\n",
    "    probas = ensemble_classifier.predict_proba(X_test)\n",
    "\n",
    "    # keep probabilities for the positive outcome only\n",
    "    probas = probas[:,1]\n",
    "\n",
    "    # calculate rocs auc scores\n",
    "    auc = roc_auc_score(y_test, probas)\n",
    "\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs, pos_label = 1)\n",
    "    fpr, tpr, _ = roc_curve(y_test, probas, pos_label = 1)\n",
    "\n",
    "    # plot the roc curve for the model\n",
    "\n",
    "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    plt.plot(fpr, tpr, marker='.', label=f\"{ensemble_classifier}, auc = {auc:.3f}\")\n",
    "\n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "\n",
    "    # show the legend\n",
    "    plt.legend(fontsize = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79722a1a-6fc5-42c0-a3da-71d6ae66dc77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(X_train, X_test, y_train, y_test, BaggingClassifier(estimator = DecisionTreeClassifier(max_depth = 7), n_estimators = 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b8b56a-41fc-4382-9087-c6e05ad3bc30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_adaboost = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 7), n_estimators = 100,\n",
    "                                  learning_rate=0.01)\n",
    "clf_adaboost.fit(X_train, y_train)\n",
    "print(f\"DT ADA boosting classifier:\\n \\\n",
    "    \\ttrain accuracy: {clf_adaboost.score(X_train, y_train):.2f}\\n \\\n",
    "    \\ttest accuracy: {clf_adaboost.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b265be20-86fa-4fa6-9df8-3a84710c0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc47473-f9d6-4c26-9b02-e7f2a7ced8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ca439-d1d8-4473-b9d2-cd2068882b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a5c7b-e460-4e90-be41-87c0dfc0b43b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rfc = RandomForestClassifier()\n",
    "# rfc_params = {'max_depth': [5,10,15,20,40,100], 'max_features': [2,3,4,5,6,7,8,9]}\n",
    "# gs_rfc = GridSearchCV(rfc, rfc_params, cv = 5, scoring='accuracy')\n",
    "# gs_rfc.fit(X_train, y_train)\n",
    "# print(gs_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d7f736-54b5-4b94-85d2-5735eceac98a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gs_rfc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057d761-1c21-4b0e-9283-9c11c2c7024d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bagging_clf = BaggingClassifier(estimator = DecisionTreeClassifier(max_depth = 7))\n",
    "bag_params = {'bootstrap': [True,False], 'bootstrap_features': [False,True],\\\n",
    "              'max_features': [6,7,8],\n",
    "                'oob_score': [True,False],\n",
    "                 'warm_start': [True]}\n",
    "\n",
    "\n",
    "gs_bag = GridSearchCV(bagging_clf, bag_params, cv = 5, scoring = 'f1')\n",
    "gs_bag.fit(X_train, y_train)\n",
    "gs_bag.best_score_\n",
    "\n",
    "# bagging_clf.get_params()\n",
    "# bagging_params = {'max_depth': [5,10,15,20,40,100], 'max_features': [2,3,4,5,6,7,8,9]}\n",
    "# gs_rfc = GridSearchCV(rfc, rfc_params, cv = 5, scoring='f1')\n",
    "# gs_rfc.fit(X_train, y_train)\n",
    "# print(gs_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f03e7c-2917-45be-94c1-b8e1e519d689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gs_bag.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90b269-73ad-4a71-b489-b694546352d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BaggingClassifier(estimator = DecisionTreeClassifier(max_depth = 7), n_estimators = 100).get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fded26-e62e-479e-9a02-cd92dc08fb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4728a9c-1e2e-423c-ac67-d6d8c796a990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74fde1-9796-425d-a4ca-2880b76b25f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# import scipy.stats as stats\n",
    "\n",
    "# Define the hyperparameter distributions\n",
    "param_dist = {\n",
    "    'max_depth': stats.randint(3, 10),\n",
    "    'learning_rate': stats.uniform(0.00001, 0.01),\n",
    "    'subsample': stats.uniform(0.5, 0.5),\n",
    "    'n_estimators':stats.randint(50, 200)\n",
    "}\n",
    "\n",
    "# Create the XGBoost model object\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(xgb_model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the RandomizedSearchCV object to the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best set of hyperparameters and the corresponding score\n",
    "print(\"Best set of hyperparameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211e3d6-d435-4f26-b7b9-f6e0d6cffecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier, \\\n",
    "    AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "f1s, precisions, recalls, accuracies, classifiers_as_str = [],[],[],[], []\n",
    "features, i_s = [], []\n",
    "# classifiers = [XGBClassifier(learning_rate = 0.009956053586134729, max_depth = 6, n_estimators = 76, subsample = 0.6838981373746974), AdaBoostClassifier,\\\n",
    "#             KNeighborsClassifier(), LogisticRegression(), DecisionTreeClassifier(), GradientBoostingClassifier(), RandomForestClassifier()]\n",
    "\n",
    "classifiers = [DecisionTreeClassifier(),KNeighborsClassifier(), LogisticRegression(),GradientBoostingClassifier(), RandomForestClassifier(),\\\n",
    "               AdaBoostClassifier(),XGBClassifier(learning_rate = 0.01, max_depth = 7, n_estimators = 100, subsample = 0.786)]\n",
    "              \n",
    "for classifier in classifiers:\n",
    "    # for i in range(1,len(feature_imp)+1):\n",
    "    #     X_train = X_train[feature_imp[:i]]\n",
    "    #     X_train.columns = [feature_imp[:i]\n",
    "    #     # X_test = X_test[feature_imp[:i]]\n",
    "    #     print(X_train)\n",
    "    #     # print(X_test)\n",
    "        \n",
    "    model = classifier\n",
    "    model.fit(X_train, y_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test,y_test_pred)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    f1s.append(f1.round(3))\n",
    "    precisions.append(precision.round(3))\n",
    "    recalls.append(recall.round(3))\n",
    "    accuracies.append(accuracy.round(3))\n",
    "    # i_s.append(i)\n",
    "    classifiers_as_str.append(str(classifier).split('(')[0])\n",
    "\n",
    "report = pd.DataFrame({'classifier': classifiers_as_str,'accuracy': accuracies, 'recall':recalls,'precision':precisions,'f1_score': f1s})\n",
    "print(report.sort_values(by = 'f1_score', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9d3e4-8f4c-4b1e-b0c0-bc13452273c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inspect the XGBClassifer confusion matrix:\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "xgb_cm = pd.DataFrame(data = confusion_matrix(y_test, y_test_pred), index = target_names, columns = target_names)\n",
    "print(xgb_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9db12-ec5e-48a4-aed5-60e80e625663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do 17-11-2023 (Raz S):\n",
    "# 1. cm of the different models\n",
    "# 2. Actual vs. predicted probas\n",
    "# 3. Look at features of false positive vs. true positive\n",
    "# 4. Balance_classes (bool)\n",
    "# 5. Class weights (Loss handling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
